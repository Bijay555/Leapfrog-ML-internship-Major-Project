{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Building.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHn8SRD_a8Wb"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k6jCk5BbBc-"
      },
      "source": [
        " This notebook consists of  different classification models to get better performance metrics for the model deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB6CKVJ2bKVc"
      },
      "source": [
        "### Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk75QUxYeFMn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "# Build Pipeline\n",
        "import joblib\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J75HxOAbaSo"
      },
      "source": [
        "### Loading the cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "oscNuBAJk6gE",
        "outputId": "070566d9-e22e-4680-edc4-90398d01aa4f"
      },
      "source": [
        "df = pd.read_csv('cleaned_data.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>cleaned_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>joy</td>\n",
              "      <td>sage act upgrade list tommorow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>way homegirl baby funeral man hate funeral sho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>joy</td>\n",
              "      <td>eye true hazel eyeand brilliant regular featur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>joy</td>\n",
              "      <td>ugh babe hugggzzz babe naamaze nga ako babe de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fear</td>\n",
              "      <td>-PRON- be expect extremely important phonecall...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentiment                                    cleaned_content\n",
              "0       joy                     sage act upgrade list tommorow\n",
              "1   sadness  way homegirl baby funeral man hate funeral sho...\n",
              "2       joy  eye true hazel eyeand brilliant regular featur...\n",
              "3       joy  ugh babe hugggzzz babe naamaze nga ako babe de...\n",
              "4      fear  -PRON- be expect extremely important phonecall..."
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpT6dYsR08AS",
        "outputId": "b3c54049-f859-4d33-a0d6-2ee7157e594e"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30631, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytg92487fO_N"
      },
      "source": [
        "### Missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_f17hmnzVNz",
        "outputId": "57ef324b-2103-4f85-e9c3-94cab79d47ff"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment           0\n",
              "cleaned_content    14\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0r3urTo0lGT"
      },
      "source": [
        "df.dropna(axis=0,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vDCUEfRyir5",
        "outputId": "57f786eb-4040-4087-8ec4-79999c91a9d6"
      },
      "source": [
        "df['sentiment'].value_counts(sort = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "joy         10395\n",
              "sadness      6128\n",
              "fear         4201\n",
              "surprise     4008\n",
              "shame        3882\n",
              "neutral      1149\n",
              "disgust       854\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIgPeKbqwBwn"
      },
      "source": [
        "### Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQfwvV3qwFxD"
      },
      "source": [
        "Lets build a quick model using TfidfVectorizer and Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_QunObRk6kz"
      },
      "source": [
        "# Features & Labels\n",
        "x_data = df['cleaned_content']\n",
        "y_data = df['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VomYqOewwFR"
      },
      "source": [
        "#  Split Data\n",
        "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3,random_state=42,stratify = y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGosV9wNwwOG"
      },
      "source": [
        "# LogisticRegression Pipeline\n",
        "pipeline_logreg = Pipeline(steps=[('tfidf',TfidfVectorizer(analyzer='word',strip_accents='unicode',ngram_range=(1, 2))),('lr',LogisticRegression(max_iter=200))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lemt7lZf_Lv"
      },
      "source": [
        "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctQafN2iwwSG",
        "outputId": "c3e35a23-0a8b-479f-afa7-bc868816c4c4"
      },
      "source": [
        "# Train and Fit Data\n",
        "pipeline_logreg.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents='unicode',\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('lr',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=200,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-pXolF9wwXl",
        "outputId": "035ba8ba-abc3-4eb7-a4f0-183083766548"
      },
      "source": [
        "# Check Accuracy and f1-score\n",
        "y_pred=pipeline_logreg.predict(x_test)\n",
        "print(pipeline_logreg.score(x_test,y_test))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5666231221423906\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     disgust       0.58      0.03      0.05       256\n",
            "        fear       0.70      0.48      0.57      1260\n",
            "         joy       0.54      0.84      0.66      3119\n",
            "     neutral       0.54      0.08      0.13       345\n",
            "     sadness       0.54      0.54      0.54      1839\n",
            "       shame       0.60      0.45      0.51      1165\n",
            "    surprise       0.61      0.36      0.45      1202\n",
            "\n",
            "    accuracy                           0.57      9186\n",
            "   macro avg       0.59      0.40      0.42      9186\n",
            "weighted avg       0.58      0.57      0.54      9186\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3f9uxNR0m0h"
      },
      "source": [
        "### Label Encoding on Sentiment data\n",
        "\n",
        "Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering.\n",
        "\n",
        "\n",
        "+ 0 -> disgust\n",
        "+ 1 -> fear\n",
        "+ 2 -> joy\n",
        "+ 3 -> neutral\n",
        "+ 4 -> sad\n",
        "+ 5 -> shame\n",
        "+ 6 -> surprise\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaloVhOdpeV0"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['sentiment'] = label_encoder.fit_transform(df['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "09_BoCdayPD4",
        "outputId": "10d5de23-b75e-4449-9185-9caf47792c60"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>cleaned_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>sage act upgrade list tommorow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>way homegirl baby funeral man hate funeral sho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>eye true hazel eyeand brilliant regular featur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>ugh babe hugggzzz babe naamaze nga ako babe de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-PRON- be expect extremely important phonecall...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                    cleaned_content\n",
              "0          2                     sage act upgrade list tommorow\n",
              "1          4  way homegirl baby funeral man hate funeral sho...\n",
              "2          2  eye true hazel eyeand brilliant regular featur...\n",
              "3          2  ugh babe hugggzzz babe naamaze nga ako babe de...\n",
              "4          1  -PRON- be expect extremely important phonecall..."
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh1fr3wZeJXm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws5HFecUxRP0"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnbA7lFAiesO"
      },
      "source": [
        "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
        "\n",
        "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-ys3fiUk6tx",
        "outputId": "ac2268bf-a56c-4fca-fcf0-d4ed90a579d1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.30, shuffle=True)\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21431, 2)\n",
            "(9186, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5H2Mi33k-L3"
      },
      "source": [
        "train_text = train['cleaned_content']\n",
        "test_text = test['cleaned_content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwfkXodblB9T"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# train, test = train_test_split(text_data, random_state=42, test_size=0.30, shuffle=True )\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
        "vectorizer.fit(train_text)\n",
        "vectorizer.fit(test_text)\n",
        "x_train = vectorizer.transform(train_text)\n",
        "y_train = train.drop(labels = ['cleaned_content'], axis=1)\n",
        "x_test = vectorizer.transform(test_text)\n",
        "y_test = test.drop(labels = ['cleaned_content'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ShOnwBbYFN"
      },
      "source": [
        "### Handling the Imbalanced dataset using SMOTE (Synthetic Minority OverSampling Techniques)\n",
        "\n",
        "SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them. SMOTE synthesises new minority instances between existing minority instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZqrtUP6ideN",
        "outputId": "01d2e8b3-a2d4-4c85-ee62-e0ed944a15fe"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNG1Eenmpme-",
        "outputId": "376a0857-3862-4e4a-a71e-c475b2c3d020"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50953, 116985), (50953,))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92geJmskzOs-",
        "outputId": "996287ad-7119-40a9-94b9-9c5b727b9e17"
      },
      "source": [
        "#1 Model - Logistic Regression\n",
        "m1 = LogisticRegression()\n",
        "m1.fit(X, y)\n",
        "pred1 = m1.predict(x_test)\n",
        "print(classification_report(y_test, pred1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.16      0.23       259\n",
            "           1       0.62      0.54      0.58      1244\n",
            "           2       0.77      0.48      0.59      3116\n",
            "           3       0.09      0.63      0.16       321\n",
            "           4       0.53      0.49      0.51      1869\n",
            "           5       0.46      0.57      0.51      1197\n",
            "           6       0.61      0.34      0.44      1180\n",
            "\n",
            "    accuracy                           0.48      9186\n",
            "   macro avg       0.50      0.46      0.43      9186\n",
            "weighted avg       0.61      0.48      0.52      9186\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJgWsAhzTdn",
        "outputId": "7774620a-fda5-434d-d52c-00397b40d9b7"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier=RandomForestClassifier()\n",
        "classifier.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjOnzEDwzTn0",
        "outputId": "7a08036c-d6bd-4de6-a5e4-0a02149892d7"
      },
      "source": [
        "y_pred=classifier.predict(x_test)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  39   24   17   60   55   50   14]\n",
            " [  10  635   72  222  165  107   33]\n",
            " [  18  188 1168  789  534  292  127]\n",
            " [   2    6   28  223   39   18    5]\n",
            " [   9  103  175  381  931  218   52]\n",
            " [   9   68   60  215  208  614   23]\n",
            " [   7   73  141  276  167   86  430]]\n",
            "0.43979969518833006\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.15      0.22       259\n",
            "           1       0.58      0.51      0.54      1244\n",
            "           2       0.70      0.37      0.49      3116\n",
            "           3       0.10      0.69      0.18       321\n",
            "           4       0.44      0.50      0.47      1869\n",
            "           5       0.44      0.51      0.48      1197\n",
            "           6       0.63      0.36      0.46      1180\n",
            "\n",
            "    accuracy                           0.44      9186\n",
            "   macro avg       0.47      0.44      0.41      9186\n",
            "weighted avg       0.56      0.44      0.47      9186\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK-GAB4-hS40"
      },
      "source": [
        "### Models and their f1 score\n",
        "\n",
        "+ Pipeline(TfidfVectorizer and Logistic Regression) -> 0.54\n",
        "+ TfidfVectorizer and Logistic Regression with Oversampling -> 0.52\n",
        "+ RandomForestClassifier with Oversampling ->0.47"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMb6UA4zet3Q"
      },
      "source": [
        "Even after oversampling the data the performance is not that better than base model.\n",
        "\n",
        "So I am dumping the base model into pickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N-WmD-rfIfC"
      },
      "source": [
        "# Save the model with pickle\n",
        "model = open(\"text_model.pkl\",\"wb\")\n",
        "joblib.dump(pipeline_logreg,model)\n",
        "model.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}